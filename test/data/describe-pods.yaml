Name:               app-65c64d7b9d-lblvb
Namespace:          default
Start Time:         Thu, 28 Feb 2019 10:11:58 -0500
Status:             Running
Controlled By:      ReplicaSet/app-65c64d7b9d
Containers:
  app:
    Restart Count:  1


Name:               cassandra-0
Namespace:          default
Start Time:         Tue, 19 Feb 2019 15:00:26 -0500
Status:             Running
Controlled By:      StatefulSet/cassandra
Containers:
  cassandra:
    Restart Count:  0


Name:               cassandra-1
Namespace:          default
Start Time:         Tue, 19 Feb 2019 15:01:33 -0500
Status:             Running
Controlled By:      StatefulSet/cassandra
Containers:
  cassandra:
    Restart Count:  0


Name:               config-6f576fc95c-fz6rr
Namespace:          default
Start Time:         Fri, 22 Feb 2019 13:21:14 -0500
Status:             Running
Controlled By:      ReplicaSet/config-6f576fc95c
Containers:
  config:
    Restart Count:  0


Name:               data-refresh-cron-1552440120-g2fsw
Start Time:         Tue, 12 Mar 2019 21:22:00 -0400
Status:             Succeeded
Controlled By:      Job/data-refresh-cron-1552440120
Containers:
  data-refresh-cron:
    Restart Count:  0


Name:               data-refresh-cron-1552440180-5gs96
Start Time:         Tue, 12 Mar 2019 21:23:00 -0400
Status:             Succeeded
Controlled By:      Job/data-refresh-cron-1552440180
Containers:
  data-refresh-cron:
    Restart Count:  0


Name:               data-refresh-cron-1552440240-l9tgv
Start Time:         Tue, 12 Mar 2019 21:24:00 -0400
Status:             Succeeded
Controlled By:      Job/data-refresh-cron-1552440240
Containers:
  data-refresh-cron:
    Restart Count:  0


Name:               elasticsearch-1-deployer-254v7
Start Time:         Fri, 22 Feb 2019 14:20:59 -0500
Status:             Succeeded
Controlled By:      Job/elasticsearch-1-deployer
Containers:
  deployer:
    Restart Count:  0


Name:               elasticsearch-1-elasticsearch-0
Start Time:         Fri, 22 Feb 2019 14:21:18 -0500
Status:             Running
Controlled By:      StatefulSet/elasticsearch-1-elasticsearch
Init Containers:
  set-max-map-count:
    Container ID:  docker://10918a6723e8d77d4c9045614823b5139c4172b9833ff4397da92c124ecc183a
    Image:         gcr.io/cloud-marketplace/google/elasticsearch/ubuntu16_04@sha256:281e570b1c254121ef9db4698554084a809d120aebfe14486c1014d0b6d4d3f5
    Image ID:      docker-pullable://gcr.io/cloud-marketplace/google/elasticsearch/ubuntu16_04@sha256:281e570b1c254121ef9db4698554084a809d120aebfe14486c1014d0b6d4d3f5
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/bash
      -c
      if [[ "$(sysctl vm.max_map_count --values)" -lt 262144 ]]; then sysctl -w vm.max_map_count=262144; fi
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 22 Feb 2019 14:21:45 -0500
      Finished:     Fri, 22 Feb 2019 14:21:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vvpc5 (ro)
Containers:
  elasticsearch:
    Container ID:   docker://214ee3606a329b77752516819470976471d3af4153272337c37b066fe78a6266
    Image:          gcr.io/cloud-marketplace/google/elasticsearch@sha256:1486a1f208579c3761a68dd12f26d0c7f4ee6728dff55a843f146c78836f7928
    Image ID:       docker-pullable://gcr.io/cloud-marketplace/google/elasticsearch@sha256:1486a1f208579c3761a68dd12f26d0c7f4ee6728dff55a843f146c78836f7928
    Ports:          9200/TCP, 9300/TCP
    Host Ports:     0/TCP, 0/TCP
    State:          Running
      Started:      Fri, 22 Feb 2019 14:22:11 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      100m
      memory:   2Gi
    Liveness:   exec [/usr/bin/pgrep -x java] delay=5s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:9200/_cluster/health%3Flocal=true delay=5s timeout=1s period=10s #success=1 #failure=3
    Environment:
      NODE_NAME:          elasticsearch-1-elasticsearch-0 (v1:metadata.name)
      CLUSTER_NAME:       elasticsearch-1-cluster
      DISCOVERY_SERVICE:  elasticsearch-1-elasticsearch-svc
      BACKUP_REPO_PATH:   
    Mounts:
      /etc/elasticsearch/elasticsearch.yml from configmap (rw)
      /etc/elasticsearch/log4j2.properties from configmap (rw)
      /usr/share/elasticsearch/data from elasticsearch-1-elasticsearch-pvc (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vvpc5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  elasticsearch-1-elasticsearch-pvc:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  elasticsearch-1-elasticsearch-pvc-elasticsearch-1-elasticsearch-0
    ReadOnly:   false
  configmap:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      elasticsearch-1-configmap
    Optional:  false
  default-token-vvpc5:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-vvpc5
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:               elasticsearch-1-elasticsearch-1
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               gke-terraform-2019-clust-default-pool-ecd4d825-zgjz/10.128.15.200
Start Time:         Fri, 22 Feb 2019 14:23:05 -0500
Labels:             app.kubernetes.io/component=elasticsearch-server
                    app.kubernetes.io/name=elasticsearch-1
                    controller-revision-hash=elasticsearch-1-elasticsearch-655475b449
                    statefulset.kubernetes.io/pod-name=elasticsearch-1-elasticsearch-1
Annotations:        kubernetes.io/limit-ranger:
                      LimitRanger plugin set: cpu request for container elasticsearch; cpu request for init container set-max-map-count
Status:             Running
IP:                 10.12.1.12
Controlled By:      StatefulSet/elasticsearch-1-elasticsearch
Init Containers:
  set-max-map-count:
    Container ID:  docker://990e4113c225140e9583809259b9820f7af5613252746a24c6e94f598f10c707
    Image:         gcr.io/cloud-marketplace/google/elasticsearch/ubuntu16_04@sha256:281e570b1c254121ef9db4698554084a809d120aebfe14486c1014d0b6d4d3f5
    Image ID:      docker-pullable://gcr.io/cloud-marketplace/google/elasticsearch/ubuntu16_04@sha256:281e570b1c254121ef9db4698554084a809d120aebfe14486c1014d0b6d4d3f5
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/bash
      -c
      if [[ "$(sysctl vm.max_map_count --values)" -lt 262144 ]]; then sysctl -w vm.max_map_count=262144; fi
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 22 Feb 2019 14:23:22 -0500
      Finished:     Fri, 22 Feb 2019 14:23:22 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vvpc5 (ro)
Containers:
  elasticsearch:
    Container ID:   docker://d8a66e083c3f0c18e74e4a6a5193331a641ebe6ed2ad1057d2ad019a7df73264
    Image:          gcr.io/cloud-marketplace/google/elasticsearch@sha256:1486a1f208579c3761a68dd12f26d0c7f4ee6728dff55a843f146c78836f7928
    Image ID:       docker-pullable://gcr.io/cloud-marketplace/google/elasticsearch@sha256:1486a1f208579c3761a68dd12f26d0c7f4ee6728dff55a843f146c78836f7928
    Ports:          9200/TCP, 9300/TCP
    Host Ports:     0/TCP, 0/TCP
    State:          Running
      Started:      Fri, 22 Feb 2019 14:23:48 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      100m
      memory:   2Gi
    Liveness:   exec [/usr/bin/pgrep -x java] delay=5s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:9200/_cluster/health%3Flocal=true delay=5s timeout=1s period=10s #success=1 #failure=3
    Environment:
      NODE_NAME:          elasticsearch-1-elasticsearch-1 (v1:metadata.name)
      CLUSTER_NAME:       elasticsearch-1-cluster
      DISCOVERY_SERVICE:  elasticsearch-1-elasticsearch-svc
      BACKUP_REPO_PATH:   
    Mounts:
      /etc/elasticsearch/elasticsearch.yml from configmap (rw)
      /etc/elasticsearch/log4j2.properties from configmap (rw)
      /usr/share/elasticsearch/data from elasticsearch-1-elasticsearch-pvc (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vvpc5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  elasticsearch-1-elasticsearch-pvc:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  elasticsearch-1-elasticsearch-pvc-elasticsearch-1-elasticsearch-1
    ReadOnly:   false
  configmap:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      elasticsearch-1-configmap
    Optional:  false
  default-token-vvpc5:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-vvpc5
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:               ingestor-858f9cccc6-cff8d
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               gke-terraform-2019-clust-default-pool-ecd4d825-zgjz/10.128.15.200
Start Time:         Thu, 28 Feb 2019 10:24:43 -0500
Labels:             app=ingestor
                    pod-template-hash=4149577772
                    role=broker
Annotations:        <none>
Status:             Running
IP:                 10.12.1.18
Controlled By:      ReplicaSet/ingestor-858f9cccc6
Containers:
  ingestor:
    Container ID:   docker://5994f896a83a143ee526160a7dc30b0b8afabac25558d10d02bd93c83f34393b
    Image:          gcr.io/taplytics-gcr/ingestor-service:apple-rfp
    Image ID:       docker-pullable://gcr.io/taplytics-gcr/ingestor-service@sha256:29c7523aa746a978ccf69fc46bcc7c885a1e6ec697d2ba83fe0eeb9bd0757746
    Port:           8090/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Thu, 28 Feb 2019 10:24:45 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:  1
    Requests:
      cpu:      1
    Liveness:   exec [./ingestor_health_check.sh] delay=60s timeout=5s period=30s #success=1 #failure=3
    Readiness:  http-get http://:8090/healthz delay=45s timeout=5s period=30s #success=1 #failure=3
    Environment Variables from:
      shared-environment  ConfigMap  Optional: false
      db-environment      ConfigMap  Optional: false
    Environment:
      WORKERTYPE:           broker
      VERTICA_CONNECTIONS:  3
      BROKER_POOL:          1
      AWS_DEFAULT_REGION:   us-east-1
      TLENV:                development
      LOG_LEVEL:            DEBUG
      PORT:                 8090
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vvpc5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-vvpc5:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-vvpc5
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:               ingestor-worker-7745b9c945-sfsml
Namespace:          default
Start Time:         Thu, 21 Feb 2019 13:59:56 -0500
Status:             Running
Controlled By:      ReplicaSet/ingestor-worker-7745b9c945
Containers:
  ingestor-worker:
    Restart Count:  0


Name:               journey-service-74c87444fb-g964c
Start Time:         Thu, 21 Feb 2019 14:01:39 -0500
Status:             Running
Controlled By:      ReplicaSet/journey-service-74c87444fb
Containers:
  journey-service:
    Restart Count:  0


Name:               kafka-0
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               gke-terraform-2019-clust-default-pool-ecd4d825-zgjz/10.128.15.200
Start Time:         Tue, 19 Feb 2019 15:00:28 -0500
Labels:             app=kafka
                    controller-revision-hash=kafka-646c5d494c
                    statefulset.kubernetes.io/pod-name=kafka-0
Annotations:        kubernetes.io/limit-ranger: LimitRanger plugin set: cpu request for container kafka
Status:             Running
IP:                 10.12.1.5
Controlled By:      StatefulSet/kafka
Containers:
  kafka:
    Container ID:  docker://65ad5f3ad933161ed1a0951eff241165b8cd37d303a54876e444a44494476d0b
    Image:         gcr.io/google_samples/k8skafka:v1
    Image ID:      docker-pullable://gcr.io/google_samples/k8skafka@sha256:1be8f40245992b94196c998d42a27da3840104c41eb78b8a389276a2c5d3b96f
    Port:          9093/TCP
    Host Port:     0/TCP
    Command:
      sh
      -c
      exec kafka-server-start.sh /opt/kafka/config/server.properties --override broker.id=${HOSTNAME##*-} \
      --override listeners=PLAINTEXT://:9093 \
      --override zookeeper.connect=zookeeper-0.zookeeper.default.svc.cluster.local:2181,zookeeper-1.zookeeper.default.svc.cluster.local:2181,zookeeper-2.zookeeper.default.svc.cluster.local:2181 \
      --override log.dir=/var/lib/kafka \
      --override auto.create.topics.enable=true \
      --override auto.leader.rebalance.enable=true \
      --override background.threads=10 \
      --override compression.type=producer \
      --override delete.topic.enable=false \
      --override leader.imbalance.check.interval.seconds=300 \
      --override leader.imbalance.per.broker.percentage=10 \
      --override log.flush.interval.messages=9223372036854775807 \
      --override log.flush.offset.checkpoint.interval.ms=60000 \
      --override log.flush.scheduler.interval.ms=9223372036854775807 \
      --override log.retention.bytes=-1 \
      --override log.retention.hours=168 \
      --override log.roll.hours=168 \
      --override log.roll.jitter.hours=0 \
      --override log.segment.bytes=1073741824 \
      --override log.segment.delete.delay.ms=60000 \
      --override message.max.bytes=1000012 \
      --override min.insync.replicas=1 \
      --override num.io.threads=8 \
      --override num.network.threads=3 \
      --override num.recovery.threads.per.data.dir=1 \
      --override num.replica.fetchers=1 \
      --override offset.metadata.max.bytes=4096 \
      --override offsets.commit.required.acks=-1 \
      --override offsets.commit.timeout.ms=5000 \
      --override offsets.load.buffer.size=5242880 \
      --override offsets.retention.check.interval.ms=600000 \
      --override offsets.retention.minutes=1440 \
      --override offsets.topic.compression.codec=0 \
      --override offsets.topic.num.partitions=50 \
      --override offsets.topic.replication.factor=3 \
      --override offsets.topic.segment.bytes=104857600 \
      --override queued.max.requests=500 \
      --override quota.consumer.default=9223372036854775807 \
      --override quota.producer.default=9223372036854775807 \
      --override replica.fetch.min.bytes=1 \
      --override replica.fetch.wait.max.ms=500 \
      --override replica.high.watermark.checkpoint.interval.ms=5000 \
      --override replica.lag.time.max.ms=10000 \
      --override replica.socket.receive.buffer.bytes=65536 \
      --override replica.socket.timeout.ms=30000 \
      --override request.timeout.ms=30000 \
      --override socket.receive.buffer.bytes=102400 \
      --override socket.request.max.bytes=104857600 \
      --override socket.send.buffer.bytes=102400 \
      --override unclean.leader.election.enable=true \
      --override zookeeper.session.timeout.ms=6000 \
      --override zookeeper.set.acl=false \
      --override broker.id.generation.enable=true \
      --override connections.max.idle.ms=600000 \
      --override controlled.shutdown.enable=true \
      --override controlled.shutdown.max.retries=3 \
      --override controlled.shutdown.retry.backoff.ms=5000 \
      --override controller.socket.timeout.ms=30000 \
      --override default.replication.factor=1 \
      --override fetch.purgatory.purge.interval.requests=1000 \
      --override group.max.session.timeout.ms=300000 \
      --override group.min.session.timeout.ms=6000 \
      --override inter.broker.protocol.version=0.10.2-IV0 \
      --override log.cleaner.backoff.ms=15000 \
      --override log.cleaner.dedupe.buffer.size=134217728 \
      --override log.cleaner.delete.retention.ms=86400000 \
      --override log.cleaner.enable=true \
      --override log.cleaner.io.buffer.load.factor=0.9 \
      --override log.cleaner.io.buffer.size=524288 \
      --override log.cleaner.io.max.bytes.per.second=1.7976931348623157E308 \
      --override log.cleaner.min.cleanable.ratio=0.5 \
      --override log.cleaner.min.compaction.lag.ms=0 \
      --override log.cleaner.threads=1 \
      --override log.cleanup.policy=delete \
      --override log.index.interval.bytes=4096 \
      --override log.index.size.max.bytes=10485760 \
      --override log.message.timestamp.difference.max.ms=9223372036854775807 \
      --override log.message.timestamp.type=CreateTime \
      --override log.preallocate=false \
      --override log.retention.check.interval.ms=300000 \
      --override max.connections.per.ip=2147483647 \
      --override num.partitions=1 \
      --override producer.purgatory.purge.interval.requests=1000 \
      --override replica.fetch.backoff.ms=1000 \
      --override replica.fetch.max.bytes=1048576 \
      --override replica.fetch.response.max.bytes=10485760 \
      --override reserved.broker.max.id=1000
      
    State:          Running
      Started:      Tue, 19 Feb 2019 15:06:46 -0500
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Tue, 19 Feb 2019 15:03:59 -0500
      Finished:     Tue, 19 Feb 2019 15:04:00 -0500
    Ready:          True
    Restart Count:  6
    Requests:
      cpu:      100m
      memory:   100M
    Liveness:   exec [sh -c /opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server=localhost:9093] delay=30s timeout=1s period=5s #success=1 #failure=3
    Readiness:  exec [sh -c /opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server=localhost:9093] delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      KAFKA_HEAP_OPTS:  -Xmx512M -Xms512M
      KAFKA_OPTS:       -Dlogging.level=INFO
    Mounts:
      /var/lib/kafka from datadir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vvpc5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  datadir:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  datadir-kafka-0
    ReadOnly:   false
  default-token-vvpc5:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-vvpc5
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:               kafka-1
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               gke-terraform-2019-clust-default-pool-ecd4d825-zhj8/10.128.15.198
Start Time:         Tue, 19 Feb 2019 15:06:53 -0500
Labels:             app=kafka
                    controller-revision-hash=kafka-646c5d494c
                    statefulset.kubernetes.io/pod-name=kafka-1
Annotations:        kubernetes.io/limit-ranger: LimitRanger plugin set: cpu request for container kafka
Status:             Running
IP:                 10.12.6.11
Controlled By:      StatefulSet/kafka
Containers:
  kafka:
    Container ID:  docker://852b3182e590ea458d8dd0b964c4700386441d758d0c0915978d468ced6db72f
    Image:         gcr.io/google_samples/k8skafka:v1
    Image ID:      docker-pullable://gcr.io/google_samples/k8skafka@sha256:1be8f40245992b94196c998d42a27da3840104c41eb78b8a389276a2c5d3b96f
    Port:          9093/TCP
    Host Port:     0/TCP
    Command:
      sh
      -c
      exec kafka-server-start.sh /opt/kafka/config/server.properties --override broker.id=${HOSTNAME##*-} \
      --override listeners=PLAINTEXT://:9093 \
      --override zookeeper.connect=zookeeper-0.zookeeper.default.svc.cluster.local:2181,zookeeper-1.zookeeper.default.svc.cluster.local:2181,zookeeper-2.zookeeper.default.svc.cluster.local:2181 \
      --override log.dir=/var/lib/kafka \
      --override auto.create.topics.enable=true \
      --override auto.leader.rebalance.enable=true \
      --override background.threads=10 \
      --override compression.type=producer \
      --override delete.topic.enable=false \
      --override leader.imbalance.check.interval.seconds=300 \
      --override leader.imbalance.per.broker.percentage=10 \
      --override log.flush.interval.messages=9223372036854775807 \
      --override log.flush.offset.checkpoint.interval.ms=60000 \
      --override log.flush.scheduler.interval.ms=9223372036854775807 \
      --override log.retention.bytes=-1 \
      --override log.retention.hours=168 \
      --override log.roll.hours=168 \
      --override log.roll.jitter.hours=0 \
      --override log.segment.bytes=1073741824 \
      --override log.segment.delete.delay.ms=60000 \
      --override message.max.bytes=1000012 \
      --override min.insync.replicas=1 \
      --override num.io.threads=8 \
      --override num.network.threads=3 \
      --override num.recovery.threads.per.data.dir=1 \
      --override num.replica.fetchers=1 \
      --override offset.metadata.max.bytes=4096 \
      --override offsets.commit.required.acks=-1 \
      --override offsets.commit.timeout.ms=5000 \
      --override offsets.load.buffer.size=5242880 \
      --override offsets.retention.check.interval.ms=600000 \
      --override offsets.retention.minutes=1440 \
      --override offsets.topic.compression.codec=0 \
      --override offsets.topic.num.partitions=50 \
      --override offsets.topic.replication.factor=3 \
      --override offsets.topic.segment.bytes=104857600 \
      --override queued.max.requests=500 \
      --override quota.consumer.default=9223372036854775807 \
      --override quota.producer.default=9223372036854775807 \
      --override replica.fetch.min.bytes=1 \
      --override replica.fetch.wait.max.ms=500 \
      --override replica.high.watermark.checkpoint.interval.ms=5000 \
      --override replica.lag.time.max.ms=10000 \
      --override replica.socket.receive.buffer.bytes=65536 \
      --override replica.socket.timeout.ms=30000 \
      --override request.timeout.ms=30000 \
      --override socket.receive.buffer.bytes=102400 \
      --override socket.request.max.bytes=104857600 \
      --override socket.send.buffer.bytes=102400 \
      --override unclean.leader.election.enable=true \
      --override zookeeper.session.timeout.ms=6000 \
      --override zookeeper.set.acl=false \
      --override broker.id.generation.enable=true \
      --override connections.max.idle.ms=600000 \
      --override controlled.shutdown.enable=true \
      --override controlled.shutdown.max.retries=3 \
      --override controlled.shutdown.retry.backoff.ms=5000 \
      --override controller.socket.timeout.ms=30000 \
      --override default.replication.factor=1 \
      --override fetch.purgatory.purge.interval.requests=1000 \
      --override group.max.session.timeout.ms=300000 \
      --override group.min.session.timeout.ms=6000 \
      --override inter.broker.protocol.version=0.10.2-IV0 \
      --override log.cleaner.backoff.ms=15000 \
      --override log.cleaner.dedupe.buffer.size=134217728 \
      --override log.cleaner.delete.retention.ms=86400000 \
      --override log.cleaner.enable=true \
      --override log.cleaner.io.buffer.load.factor=0.9 \
      --override log.cleaner.io.buffer.size=524288 \
      --override log.cleaner.io.max.bytes.per.second=1.7976931348623157E308 \
      --override log.cleaner.min.cleanable.ratio=0.5 \
      --override log.cleaner.min.compaction.lag.ms=0 \
      --override log.cleaner.threads=1 \
      --override log.cleanup.policy=delete \
      --override log.index.interval.bytes=4096 \
      --override log.index.size.max.bytes=10485760 \
      --override log.message.timestamp.difference.max.ms=9223372036854775807 \
      --override log.message.timestamp.type=CreateTime \
      --override log.preallocate=false \
      --override log.retention.check.interval.ms=300000 \
      --override max.connections.per.ip=2147483647 \
      --override num.partitions=1 \
      --override producer.purgatory.purge.interval.requests=1000 \
      --override replica.fetch.backoff.ms=1000 \
      --override replica.fetch.max.bytes=1048576 \
      --override replica.fetch.response.max.bytes=10485760 \
      --override reserved.broker.max.id=1000
      
    State:          Running
      Started:      Tue, 19 Feb 2019 15:07:18 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      100m
      memory:   100M
    Liveness:   exec [sh -c /opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server=localhost:9093] delay=30s timeout=1s period=5s #success=1 #failure=3
    Readiness:  exec [sh -c /opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server=localhost:9093] delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      KAFKA_HEAP_OPTS:  -Xmx512M -Xms512M
      KAFKA_OPTS:       -Dlogging.level=INFO
    Mounts:
      /var/lib/kafka from datadir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vvpc5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  datadir:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  datadir-kafka-1
    ReadOnly:   false
  default-token-vvpc5:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-vvpc5
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:               kafka-2
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               gke-terraform-2019-clust-default-pool-ecd4d825-m9kn/10.128.15.195
Start Time:         Tue, 19 Feb 2019 15:07:31 -0500
Labels:             app=kafka
                    controller-revision-hash=kafka-646c5d494c
                    statefulset.kubernetes.io/pod-name=kafka-2
Annotations:        kubernetes.io/limit-ranger: LimitRanger plugin set: cpu request for container kafka
Status:             Running
IP:                 10.12.4.9
Controlled By:      StatefulSet/kafka
Containers:
  kafka:
    Container ID:  docker://6622fe9fcdbe0359a6fff5333d212a83716bd768f23a7afb9c82d83b3ef951cb
    Image:         gcr.io/google_samples/k8skafka:v1
    Image ID:      docker-pullable://gcr.io/google_samples/k8skafka@sha256:1be8f40245992b94196c998d42a27da3840104c41eb78b8a389276a2c5d3b96f
    Port:          9093/TCP
    Host Port:     0/TCP
    Command:
      sh
      -c
      exec kafka-server-start.sh /opt/kafka/config/server.properties --override broker.id=${HOSTNAME##*-} \
      --override listeners=PLAINTEXT://:9093 \
      --override zookeeper.connect=zookeeper-0.zookeeper.default.svc.cluster.local:2181,zookeeper-1.zookeeper.default.svc.cluster.local:2181,zookeeper-2.zookeeper.default.svc.cluster.local:2181 \
      --override log.dir=/var/lib/kafka \
      --override auto.create.topics.enable=true \
      --override auto.leader.rebalance.enable=true \
      --override background.threads=10 \
      --override compression.type=producer \
      --override delete.topic.enable=false \
      --override leader.imbalance.check.interval.seconds=300 \
      --override leader.imbalance.per.broker.percentage=10 \
      --override log.flush.interval.messages=9223372036854775807 \
      --override log.flush.offset.checkpoint.interval.ms=60000 \
      --override log.flush.scheduler.interval.ms=9223372036854775807 \
      --override log.retention.bytes=-1 \
      --override log.retention.hours=168 \
      --override log.roll.hours=168 \
      --override log.roll.jitter.hours=0 \
      --override log.segment.bytes=1073741824 \
      --override log.segment.delete.delay.ms=60000 \
      --override message.max.bytes=1000012 \
      --override min.insync.replicas=1 \
      --override num.io.threads=8 \
      --override num.network.threads=3 \
      --override num.recovery.threads.per.data.dir=1 \
      --override num.replica.fetchers=1 \
      --override offset.metadata.max.bytes=4096 \
      --override offsets.commit.required.acks=-1 \
      --override offsets.commit.timeout.ms=5000 \
      --override offsets.load.buffer.size=5242880 \
      --override offsets.retention.check.interval.ms=600000 \
      --override offsets.retention.minutes=1440 \
      --override offsets.topic.compression.codec=0 \
      --override offsets.topic.num.partitions=50 \
      --override offsets.topic.replication.factor=3 \
      --override offsets.topic.segment.bytes=104857600 \
      --override queued.max.requests=500 \
      --override quota.consumer.default=9223372036854775807 \
      --override quota.producer.default=9223372036854775807 \
      --override replica.fetch.min.bytes=1 \
      --override replica.fetch.wait.max.ms=500 \
      --override replica.high.watermark.checkpoint.interval.ms=5000 \
      --override replica.lag.time.max.ms=10000 \
      --override replica.socket.receive.buffer.bytes=65536 \
      --override replica.socket.timeout.ms=30000 \
      --override request.timeout.ms=30000 \
      --override socket.receive.buffer.bytes=102400 \
      --override socket.request.max.bytes=104857600 \
      --override socket.send.buffer.bytes=102400 \
      --override unclean.leader.election.enable=true \
      --override zookeeper.session.timeout.ms=6000 \
      --override zookeeper.set.acl=false \
      --override broker.id.generation.enable=true \
      --override connections.max.idle.ms=600000 \
      --override controlled.shutdown.enable=true \
      --override controlled.shutdown.max.retries=3 \
      --override controlled.shutdown.retry.backoff.ms=5000 \
      --override controller.socket.timeout.ms=30000 \
      --override default.replication.factor=1 \
      --override fetch.purgatory.purge.interval.requests=1000 \
      --override group.max.session.timeout.ms=300000 \
      --override group.min.session.timeout.ms=6000 \
      --override inter.broker.protocol.version=0.10.2-IV0 \
      --override log.cleaner.backoff.ms=15000 \
      --override log.cleaner.dedupe.buffer.size=134217728 \
      --override log.cleaner.delete.retention.ms=86400000 \
      --override log.cleaner.enable=true \
      --override log.cleaner.io.buffer.load.factor=0.9 \
      --override log.cleaner.io.buffer.size=524288 \
      --override log.cleaner.io.max.bytes.per.second=1.7976931348623157E308 \
      --override log.cleaner.min.cleanable.ratio=0.5 \
      --override log.cleaner.min.compaction.lag.ms=0 \
      --override log.cleaner.threads=1 \
      --override log.cleanup.policy=delete \
      --override log.index.interval.bytes=4096 \
      --override log.index.size.max.bytes=10485760 \
      --override log.message.timestamp.difference.max.ms=9223372036854775807 \
      --override log.message.timestamp.type=CreateTime \
      --override log.preallocate=false \
      --override log.retention.check.interval.ms=300000 \
      --override max.connections.per.ip=2147483647 \
      --override num.partitions=1 \
      --override producer.purgatory.purge.interval.requests=1000 \
      --override replica.fetch.backoff.ms=1000 \
      --override replica.fetch.max.bytes=1048576 \
      --override replica.fetch.response.max.bytes=10485760 \
      --override reserved.broker.max.id=1000
      
    State:          Running
      Started:      Tue, 19 Feb 2019 15:07:55 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      100m
      memory:   100M
    Liveness:   exec [sh -c /opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server=localhost:9093] delay=30s timeout=1s period=5s #success=1 #failure=3
    Readiness:  exec [sh -c /opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server=localhost:9093] delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      KAFKA_HEAP_OPTS:  -Xmx512M -Xms512M
      KAFKA_OPTS:       -Dlogging.level=INFO
    Mounts:
      /var/lib/kafka from datadir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vvpc5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  datadir:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  datadir-kafka-2
    ReadOnly:   false
  default-token-vvpc5:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-vvpc5
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:               message-delivery-7f6b979d94-ttqqn
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               gke-terraform-2019-clust-default-pool-ecd4d825-wpr7/10.128.15.197
Start Time:         Thu, 21 Feb 2019 14:00:16 -0500
Labels:             app=message-delivery
                    org=taplytics
                    pod-template-hash=3926535850
Annotations:        <none>
Status:             Running
IP:                 10.12.3.12
Controlled By:      ReplicaSet/message-delivery-7f6b979d94
Containers:
  message-delivery:
    Container ID:   docker://3417e7af1342de1bddf1e6246c9a5c3d0fda98163bdd9f03957cc4680b429e3e
    Image:          gcr.io/taplytics-gcr/message-delivery-service:apple-rfp
    Image ID:       docker-pullable://gcr.io/taplytics-gcr/message-delivery-service@sha256:3c24ceadc1a3703d3649bcf338cb7685605d6e820148f7219b7c8072d54cff85
    Port:           3009/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Thu, 21 Feb 2019 14:00:49 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     2
      memory:  1200M
    Requests:
      cpu:      2
      memory:   600M
    Liveness:   http-get http://:3009/healthz delay=10s timeout=5s period=10s #success=1 #failure=3
    Readiness:  http-get http://:3009/healthz delay=10s timeout=5s period=10s #success=1 #failure=3
    Environment Variables from:
      shared-environment  ConfigMap  Optional: false
      db-environment      ConfigMap  Optional: false
    Environment:
      PORT:                  3009
      PELTON_BATCH_SIZE:     100
      BATCH_FLUSH_INTERVAL:  200
      QUEUE_BATCH_SIZE:      100
      BUFFER_SIZE:           220
      MAX_RETRIES:           0
      LOG_LEVEL:             debug
      ETS_DELIVERY:          1
      CPU_COUNT:             2 (requests.cpu)
      NODE_ENV:              development
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vvpc5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-vvpc5:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-vvpc5
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:               react-dashboard-74dfb8b75b-zwbgm
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               gke-terraform-2019-clust-default-pool-ecd4d825-zhj8/10.128.15.198
Start Time:         Thu, 07 Mar 2019 10:49:36 -0500
Labels:             app=react-dashboard
                    org=taplytics
                    pod-template-hash=3089646316
Annotations:        <none>
Status:             Running
IP:                 10.12.6.18
Controlled By:      ReplicaSet/react-dashboard-74dfb8b75b
Containers:
  react-dashboard:
    Container ID:   docker://f4fc82e193d1953195c15d46ed3a0d3999334ee49f58d65c8b5dea4b9f02f101
    Image:          gcr.io/taplytics-gcr/react-dashboard:apple-rfp
    Image ID:       docker-pullable://gcr.io/taplytics-gcr/react-dashboard@sha256:9366c5e82d90a229f849b5cd4d27da599201ca41cfa16ba29324f9cc3c89216b
    Port:           3000/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Thu, 07 Mar 2019 10:49:37 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  2G
    Requests:
      cpu:     1
      memory:  1G
    Environment:
      REACT_RECURRENCE_MINIMUM:  1
      UPLOADCARE_KEY:            ef3d34b068f04e7c04c7
      REACT_ENABLE_RECURRENCE:   1
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vvpc5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-vvpc5:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-vvpc5
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:               redisbox-7b589fbf56-j6p9f
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               gke-terraform-2019-clust-default-pool-ecd4d825-b5px/10.128.15.199
Start Time:         Thu, 21 Feb 2019 10:12:16 -0500
Labels:             pod-template-hash=3614596912
                    run=redisbox
Annotations:        kubernetes.io/limit-ranger: LimitRanger plugin set: cpu request for container redisbox
Status:             Running
IP:                 10.12.2.12
Controlled By:      ReplicaSet/redisbox-7b589fbf56
Containers:
  redisbox:
    Container ID:  docker://0fd055217973bc56fb234b7e82ffbb5a6ad97834005d6006102d2d29099bfb14
    Image:         gcr.io/google_containers/redis:v1
    Image ID:      docker-pullable://gcr.io/google_containers/redis@sha256:ae4699b8f330d61665125cd381942fc1c57613f2753c5ee6879aff173a8ccb52
    Port:          <none>
    Host Port:     <none>
    Args:
      sh
    State:          Running
      Started:      Thu, 21 Feb 2019 10:19:35 -0500
    Last State:     Terminated
      Reason:       Error
      Exit Code:    130
      Started:      Thu, 21 Feb 2019 10:12:25 -0500
      Finished:     Thu, 21 Feb 2019 10:19:34 -0500
    Ready:          True
    Restart Count:  1
    Requests:
      cpu:        100m
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vvpc5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-vvpc5:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-vvpc5
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:               scheduler-6d497d6895-822c6
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               gke-terraform-2019-clust-default-pool-ecd4d825-m9kn/10.128.15.195
Start Time:         Thu, 21 Feb 2019 14:00:36 -0500
Labels:             app=scheduler
                    org=taplytics
                    pod-template-hash=2805382451
                    role=broker
Annotations:        <none>
Status:             Running
IP:                 10.12.4.17
Controlled By:      ReplicaSet/scheduler-6d497d6895
Containers:
  scheduler:
    Container ID:   docker://8a44fc9bc7ed8d5fa7f6afab09c0d5298c59d2f1d767b2648719b743e8bccf29
    Image:          gcr.io/taplytics-gcr/scheduling-service:apple-rfp
    Image ID:       docker-pullable://gcr.io/taplytics-gcr/scheduling-service@sha256:d3ad029f803f4135be036c4770f2ff7dd6889dc92bfb6fb8dd2c4aa5d134ea95
    Port:           3000/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Thu, 21 Feb 2019 14:01:05 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  300Mi
    Requests:
      cpu:      300m
      memory:   300Mi
    Liveness:   http-get http://:3000/healthz delay=20s timeout=5s period=10s #success=1 #failure=3
    Readiness:  http-get http://:3000/healthz delay=20s timeout=5s period=10s #success=1 #failure=3
    Environment Variables from:
      shared-environment  ConfigMap  Optional: false
      db-environment      ConfigMap  Optional: false
    Environment:
      MAX_CONNS:  1
      CPU_COUNT:  1 (requests.cpu)
      PORT:       3000
      LOG_LEVEL:  info
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vvpc5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-vvpc5:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-vvpc5
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:               scheduler-worker-66c4657b5c-4nsh6
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               gke-terraform-2019-clust-default-pool-ecd4d825-b5px/10.128.15.199
Start Time:         Thu, 21 Feb 2019 14:00:45 -0500
Labels:             app=scheduler
                    org=taplytics
                    pod-template-hash=2270213617
                    role=worker
Annotations:        <none>
Status:             Running
IP:                 10.12.2.17
Controlled By:      ReplicaSet/scheduler-worker-66c4657b5c
Containers:
  scheduler-worker:
    Container ID:   docker://28f9df7fb3f44001b83dc3f9a9267f17db056bcd81493e3c718fc0672b9faf15
    Image:          gcr.io/taplytics-gcr/scheduling-service:apple-rfp
    Image ID:       docker-pullable://gcr.io/taplytics-gcr/scheduling-service@sha256:d3ad029f803f4135be036c4770f2ff7dd6889dc92bfb6fb8dd2c4aa5d134ea95
    Port:           3000/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Thu, 21 Feb 2019 14:01:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  1Gi
    Requests:
      cpu:     300m
      memory:  1Gi
    Environment Variables from:
      shared-environment  ConfigMap  Optional: false
      db-environment      ConfigMap  Optional: false
    Environment:
      WORKER:       1
      LOG_LEVEL:    info
      SKIP_KAFKA:   0
      CPU_COUNT:    1
      MAX_WORKERS:  5
      PORT:         3000
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vvpc5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-vvpc5:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-vvpc5
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:               vertica-0
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               gke-terraform-2019-clust-default-pool-ecd4d825-b5px/10.128.15.199
Start Time:         Tue, 26 Feb 2019 15:13:48 -0500
Labels:             app=vertica
                    controller-revision-hash=vertica-67687b89bb
                    statefulset.kubernetes.io/pod-name=vertica-0
Annotations:        <none>
Status:             Running
IP:                 10.12.2.19
Controlled By:      StatefulSet/vertica
Containers:
  vertica:
    Container ID:   docker://4d50966347472c34b007d783ff52ab7600cfd0fb87fcca2e8315d024e25e7013
    Image:          gcr.io/taplytics-gcr/vertica:9.1.1
    Image ID:       docker-pullable://gcr.io/taplytics-gcr/vertica@sha256:8e0ac4f396388f4a53f4a6a36aa83b319738154c42256f1f86acf05bf645b636
    Port:           5433/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Tue, 26 Feb 2019 15:15:42 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
      memory:     5G
    Environment:  <none>
    Mounts:
      /opt/vertica from datadir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vvpc5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  datadir:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  datadir-vertica-0
    ReadOnly:   false
  default-token-vvpc5:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-vvpc5
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:               vertica-console-0
Namespace:          default
Start Time:         Tue, 26 Feb 2019 15:16:48 -0500
Status:             Pending
Controlled By:      StatefulSet/vertica-console
Containers:
  vertica-console:
    Restart Count:  0


Name:               zookeeper-0
Start Time:         Tue, 19 Feb 2019 15:00:22 -0500
Status:             Running
Controlled By:      StatefulSet/zookeeper
Containers:
  zookeeper:
    Restart Count:  0


Name:               zookeeper-1
Start Time:         Tue, 19 Feb 2019 15:02:49 -0500
Status:             Running
Controlled By:      StatefulSet/zookeeper
Containers:
  zookeeper:
    Restart Count:  0


Name:               zookeeper-2
Namespace:          default
Start Time:         Tue, 19 Feb 2019 15:03:31 -0500
Status:             Running
Controlled By:      StatefulSet/zookeeper
Containers:
  zookeeper:
    Restart Count:  0
